































































































































































































































































































100%|██████████| 291/291 [11:24<00:00,  2.35s/it]
{'train_runtime': 685.8232, 'train_samples_per_second': 6.316, 'train_steps_per_second': 0.424, 'train_loss': 0.5545621262383216, 'epoch': 3.0}





















100%|██████████| 23/23 [00:54<00:00,  2.35s/it]




100%|██████████| 6/6 [00:11<00:00,  1.97s/it]
fc1_dim:1713
dropout_rate0:0.2936226199061334
dropout_rate1:0.11310363207508646
weight_decay:3.5641097878046897
lr:4.82042749786395e-05
warmup_steps:548
per_batch_size:15
Train Accuracy: 0.8732686980609419
Val Accuracy: 0.7624309392265194
################### FINISHED TRIAL 0 ###################
################### STARTED TRIAL 1 ###################
[I 2023-07-09 18:52:18,680] Trial 0 finished with value: 0.7624309392265194 and parameters: {'fc1_dim': 1713, 'dropout_rate0': 0.2936226199061334, 'dropout_rate1': 0.11310363207508646, 'weight_decay': 3.5641097878046897, 'lr': 4.82042749786395e-05, 'warmup_steps': 548, 'per_batch_size': 15}. Best is trial 0 with value: 0.7624309392265194.
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing CustomDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing CustomDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CustomDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CustomDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.0.bias', 'classifier.3.bias', 'classifier.3.weight', 'classifier.0.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(































































































































































































































































































































































100%|█████████▉| 433/435 [11:59<00:02,  1.41s/it]

100%|██████████| 435/435 [12:01<00:00,  1.66s/it]





















100%|██████████| 23/23 [00:47<00:00,  2.05s/it]




 83%|████████▎ | 5/6 [00:07<00:01,  1.67s/it]
fc1_dim:1031
dropout_rate0:0.06976635888594955
dropout_rate1:0.4722478060351426
weight_decay:3.9917877471799637
lr:0.0002827096850541622
warmup_steps:706
per_batch_size:10
Train Accuracy: 0.8282548476454293
Val Accuracy: 0.6850828729281768
################### FINISHED TRIAL 1 ###################
100%|██████████| 6/6 [00:08<00:00,  1.48s/it]
[I 2023-07-09 19:05:22,840] Trial 1 finished with value: 0.6850828729281768 and parameters: {'fc1_dim': 1031, 'dropout_rate0': 0.06976635888594955, 'dropout_rate1': 0.4722478060351426, 'weight_decay': 3.9917877471799637, 'lr': 0.0002827096850541622, 'warmup_steps': 706, 'per_batch_size': 10}. Best is trial 0 with value: 0.7624309392265194.
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing CustomDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing CustomDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CustomDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CustomDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.0.bias', 'classifier.3.bias', 'classifier.3.weight', 'classifier.0.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(





















































































































































 99%|█████████▉| 149/150 [07:48<00:03,  3.16s/it]

100%|██████████| 150/150 [07:50<00:00,  3.14s/it]





















100%|██████████| 23/23 [00:46<00:00,  2.02s/it]




 83%|████████▎ | 5/6 [00:07<00:01,  1.71s/it]
fc1_dim:211
dropout_rate0:0.16457265749470043
dropout_rate1:0.20082161176430657
weight_decay:2.9665635217785318
lr:1.7344535141555305e-05
warmup_steps:484
per_batch_size:29
Train Accuracy: 0.7409972299168975
Val Accuracy: 0.7430939226519337
################### FINISHED TRIAL 2 ###################
100%|██████████| 6/6 [00:09<00:00,  1.51s/it]
[I 2023-07-09 19:14:14,946] Trial 2 finished with value: 0.7430939226519337 and parameters: {'fc1_dim': 211, 'dropout_rate0': 0.16457265749470043, 'dropout_rate1': 0.20082161176430657, 'weight_decay': 2.9665635217785318, 'lr': 1.7344535141555305e-05, 'warmup_steps': 484, 'per_batch_size': 29}. Best is trial 0 with value: 0.7624309392265194.
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing CustomDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing CustomDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CustomDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CustomDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.0.bias', 'classifier.3.bias', 'classifier.3.weight', 'classifier.0.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(






































































































 90%|█████████ | 103/114 [06:59<00:47,  4.29s/it][W 2023-07-09 19:21:20,092] Trial 3 failed with parameters: {'fc1_dim': 804, 'dropout_rate0': 0.45139239538938425, 'dropout_rate1': 0.5244689541024332, 'weight_decay': 2.475801371854676, 'lr': 9.763497192205395e-05, 'warmup_steps': 548, 'per_batch_size': 39} because of the following error: KeyboardInterrupt().
Traceback (most recent call last):
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\optuna\study\_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "C:\Users\user\OneDrive - Technion\GoodNotes\Semester 8\Deep Learning\project\code\HumanOrGPT\Models\BasedOnTrainer.py", line 185, in objective
    trainer.train()
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1645, in train
    return inner_training_loop(
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\accelerate\accelerator.py", line 1821, in backward
    loss.backward(**kwargs)
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[W 2023-07-09 19:21:20,141] Trial 3 failed with value None.
Traceback (most recent call last):
  File "C:\Users\user\OneDrive - Technion\GoodNotes\Semester 8\Deep Learning\project\code\HumanOrGPT\Models\BasedOnTrainer.py", line 233, in <module>
  File "C:\Users\user\OneDrive - Technion\GoodNotes\Semester 8\Deep Learning\project\code\HumanOrGPT\Models\BasedOnTrainer.py", line 222, in custom_model_on_rephrased
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\optuna\study\study.py", line 443, in optimize
    _optimize(
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\optuna\study\_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\optuna\study\_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\optuna\study\_optimize.py", line 251, in _run_trial
    raise func_err
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\optuna\study\_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "C:\Users\user\OneDrive - Technion\GoodNotes\Semester 8\Deep Learning\project\code\HumanOrGPT\Models\BasedOnTrainer.py", line 185, in objective
    trainer.train()
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1645, in train
    return inner_training_loop(
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\accelerate\accelerator.py", line 1821, in backward
    loss.backward(**kwargs)
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt